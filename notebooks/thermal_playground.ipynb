{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thermal Alert \n",
    "\n",
    "\n",
    "### The Gist \n",
    "\n",
    "A PostgreSQL database is hosted on Kubernetes\n",
    "\n",
    "- **host**: bayleef.wr.usgs.gov\n",
    "- **port**: 54320\n",
    "- **owner**: kelvin \n",
    "- **password**: 1234\n",
    "- **database**: thermal\n",
    "\n",
    "PGAdmin is running @ `bayleef.wr.usgs.gov`\n",
    "\n",
    "Each dataset has it's own schema, currently the only one is LANDSAT_8_C1.\n",
    "More to come soon\n",
    "\n",
    "<html>\n",
    "  <head>\n",
    "    <title></title>\n",
    "    <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\">\n",
    "    <style type=\"text/css\">\n",
    "      body { font-family: verdana, arial, helvetica, sans-serif; font-size: smaller; }\n",
    "      table { padding: 0px;  margin: 0px; border: 1px solid #dddddd; border-collapse: collapse; }\n",
    "      tr { font-size: smaller; }\n",
    "      th, td { border: 1px solid #dddddd; padding: 2px; }\n",
    "      /*table.info { background: #ffffff; }*/\n",
    "      /*table.data { background: #ffffff; }*/\n",
    "      /*tr.even { background: #ffffff; }*/\n",
    "      /*tr.odd { background: #eeeeee; }*/\n",
    "    </style>\n",
    "  </head>\n",
    "  <body>\n",
    "    <br>\n",
    "    <table class=\"data\">\n",
    "      <tr>\n",
    "        <th>table_schem</th>\n",
    "        <th>table_name</th>\n",
    "        <th>table_type</th>\n",
    "        <th>remarks</th>\n",
    "      </tr>\n",
    "      <tr class=\"even\">\n",
    "        <td>\"landsat_8_c1\"</td>\n",
    "        <td>\"image_attributes\"</td>\n",
    "        <td>\"TABLE\"</td>\n",
    "        <td>\"Contains misc attributes such as cloud cover, sun azimuth and saturated band info. \"</td>\n",
    "      </tr>\n",
    "      <tr class=\"odd\">\n",
    "        <td>\"landsat_8_c1\"</td>\n",
    "        <td>\"images\"</td>\n",
    "        <td>\"TABLE\"</td>\n",
    "        <td>\"Contains file path to band geotiffs and metadata files along with a spatiotemporal index.  \"</td>\n",
    "      </tr>\n",
    "      <tr class=\"even\">\n",
    "        <td>\"landsat_8_c1\"</td>\n",
    "        <td>\"metadata_file_info\"</td>\n",
    "        <td>\"TABLE\"</td>\n",
    "        <td>\"Landsat 8 c1 specific file id&#39;s and bookkeeping info \"</td>\n",
    "      </tr>\n",
    "      <tr class=\"odd\">\n",
    "        <td>\"landsat_8_c1\"</td>\n",
    "        <td>\"min_max_pixel_value\"</td>\n",
    "        <td>\"TABLE\"</td>\n",
    "        <td>N/A</td>\n",
    "      </tr>\n",
    "      <tr class=\"even\">\n",
    "        <td>\"landsat_8_c1\"</td>\n",
    "        <td>\"min_max_radiance\"</td>\n",
    "        <td>\"TABLE\"</td>\n",
    "        <td>N/A</td>\n",
    "      </tr>\n",
    "      <tr class=\"odd\">\n",
    "        <td>\"landsat_8_c1\"</td>\n",
    "        <td>\"min_max_reflectance\"</td>\n",
    "        <td>\"TABLE\"</td>\n",
    "        <td>N/A</td>\n",
    "      </tr>\n",
    "      <tr class=\"even\">\n",
    "        <td>\"landsat_8_c1\"</td>\n",
    "        <td>\"product_metadata\"</td>\n",
    "        <td>\"TABLE\"</td>\n",
    "        <td>\"Spatial info include lat lon extents, row&#47;path, and  file names. \"</td>\n",
    "      </tr>\n",
    "      <tr class=\"odd\">\n",
    "        <td>\"landsat_8_c1\"</td>\n",
    "        <td>\"projection_parameters\"</td>\n",
    "        <td>\"TABLE\"</td>\n",
    "        <td>N/A</td>\n",
    "      </tr>\n",
    "      <tr class=\"even\">\n",
    "        <td>\"landsat_8_c1\"</td>\n",
    "        <td>\"radiometric_rescaling\"</td>\n",
    "        <td>\"TABLE\"</td>\n",
    "        <td>\"Columns containing constants for converting to and from reflectance and radiance.\"</td>\n",
    "      </tr>\n",
    "      <tr class=\"odd\">\n",
    "        <td>\"landsat_8_c1\"</td>\n",
    "        <td>\"tirs_thermal_constants\"</td>\n",
    "        <td>\"TABLE\"</td>\n",
    "        <td>\"K1 and K2 constants for converting to and from brightness temp.\"</td>\n",
    "      </tr>\n",
    "    </table>\n",
    "  </body>\n",
    "</html>\n",
    "\n",
    "All the functions written below make the assumption that the in-memory dataframes maintain the same format as the tables above (hit up `bayleef.wr.usgs.gov` and look through PGAdmin if you want to get specific column info).  \n",
    "\n",
    "### Notes\n",
    "* It seems like USGS EROS doesn't tag their day/night images correctly and some daytime images being downloaded despite specifically querying for night time. After speaking with Greg, seems I can filter based on row/path instead of relying on USGS tagging. \n",
    "* The algorithm given to me by Greg for brightness temp didn't seem correct. I got values in the ~-500 K range. Using the algorithm given by USGS EROS (https://landsat.usgs.gov/using-usgs-landsat-8-product) seems to give brightness temps arround ~300 K which I believe is more expected? Need someone to check my math here. \n",
    "* VAST was written by Cole, but I haven't gotten my hands on the function yet, it's finals week so it's reasonable. \n",
    "* A lot of this is temporary, since the underlying data structures are likely to change. The question of a good data structure is a bit of a challenge at the moment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import plio\n",
    "from plio.io.io_gdal import GeoDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.animation as animation\n",
    "from pylab import rcParams\n",
    "import numpy as np\n",
    "import gdal \n",
    "from os import path\n",
    "import os\n",
    "import math\n",
    "import osr \n",
    "import hashlib\n",
    "import pvl\n",
    "from glob import glob \n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "from geoalchemy2 import Geometry, WKTElement\n",
    "from geoalchemy2.shape import from_shape\n",
    "import shapely\n",
    "from shapely.geometry import Polygon\n",
    "from sqlalchemy import *\n",
    "import re\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from IPython.display import HTML\n",
    "\n",
    "rcParams['figure.figsize'] = (20,10)\n",
    "landsat8_wavelengths = [None, 0.435, \n",
    "                        0.452, 0.533, \n",
    "                        0.636, 0.851,\n",
    "                        1.566, 2.107,\n",
    "                        0.503, 1.363,\n",
    "                        11.19, 12.51]\n",
    "\n",
    "\n",
    "from ipywidgets import interactive,interact, fixed, Layout, IntSlider\n",
    "\n",
    "def dfshow(df, cmap='coolwarm'):\n",
    "    def vis_col(df, row, col):\n",
    "        plt.imshow(df[col].iloc[row].read_array(), interpolation='None', cmap=cmap)\n",
    "        plt.colorbar()\n",
    "        plt.title('{} : {}'.format(df.reset_index().iloc[row].landsat_scene_id, df.iloc[row].time))\n",
    "        plt.show()\n",
    "    \n",
    "    def get_data_cols(df):\n",
    "        data_columns = []\n",
    "        sample = df.iloc[0]\n",
    "        for column in sample.index:\n",
    "            if isinstance(sample[column], GeoDataset):\n",
    "                data_columns.append(column)\n",
    "        return data_columns\n",
    "    \n",
    "    num_rows = df.shape[0]-1\n",
    "    cols = get_data_cols(df)\n",
    "    \n",
    "    return interactive(vis_col, df=fixed(df), row=IntSlider(min=0,max=num_rows, layout=Layout(width='50%')), col=cols)\n",
    "\n",
    "\n",
    "def modvolc(mir, tir, thresh=-.8, nodata=0):\n",
    "    \"\"\"\n",
    "    Modvolc computation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mir : np.array \n",
    "          array containing mid infrared DNs for computing nti\n",
    "    tir : np.array \n",
    "          array containing thermal infrared DNs for computing nti\n",
    "    thresh : float\n",
    "             Pixels where nti >= tresh are tagged as anomolies \n",
    "    nodata : float \n",
    "             DNs in mir and tir equal to the no data values are replaced \n",
    "             with np.nan\n",
    "             \n",
    "    Returns\n",
    "    -------\n",
    "    : np.array \n",
    "      Boolean array of pixels flagged as anomolies \n",
    "    : np.array\n",
    "      Computed nti array where nti = (mir - tir)/(mir + tir)\n",
    "    \n",
    "    \"\"\"\n",
    "    tir[tir == nodata] = np.nan\n",
    "    mir[mir == nodata] = np.nan\n",
    "    nti = (mir - tir)/(mir + tir)\n",
    "    \n",
    "    anomolies = np.empty(nti.shape)\n",
    "    anomolies[:] = False\n",
    "    anomolies[np.isnan(nti)] = np.nan\n",
    "    anomolies[nti >= thresh] = True\n",
    "    return anomolies, nti\n",
    "    \n",
    "    \n",
    "\n",
    "def modvolc_df(df, mir_band, tir_band, thresh=-.8):\n",
    "    \"\"\"\n",
    "    Runs the modvolc algorithm on a pandas dataframe, assumes format matches the \n",
    "    postgres database.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    df : DataFrame\n",
    "         Pandas DataFrame to apply modvolc to\n",
    "    mir : str\n",
    "          Mid infrared column, this is the column that contains the image\n",
    "          used as the mir band, usually b6 or b7\n",
    "    tir : str\n",
    "          Thermal infrared column,this is the column that contains the image\n",
    "          used as the tir band, usually b10 or b11\n",
    "    thresh : float \n",
    "             Threshhold to be passed to modvolc, where anomolies == where(nti >= thresh)\n",
    "    \n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    \n",
    "    : DataFrame\n",
    "      Dataframe with new columns \"modvolc_anomolies\" and \"nti\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def modvolc_row(row, mir_band, tir_band):\n",
    "        mir_arr = row[mir_band].read_array()\n",
    "        tir_arr = row[tir_band].read_array()\n",
    "        \n",
    "        anomolies, nti = modvolc(mir_arr, tir_arr, thresh)\n",
    "        anomolies = array2raster(row[tir_band], anomolies, os.path.join('/vsimem',hash_dataset(anomolies)))\n",
    "        nti = array2raster(row[tir_band], nti, os.path.join('/vsimem', hash_dataset(nti)))\n",
    "        row['modvolc_anomolies'], row['nti'] = anomolies, nti\n",
    "        return row\n",
    "    return df.apply(modvolc_row, axis=1, mir_band=mir_band, tir_band=tir_band)\n",
    "    \n",
    "\n",
    "def hash_dataset(arr=None):\n",
    "    \"\"\"\n",
    "    Hashes an array. Values are rounded to one decimal place so to avoid \n",
    "    issues of floating point inaccuracies. Useful for programtically generating \n",
    "    filenames for GDAL files. As GDAL file are kept in memory using GDAL's virtual file system \n",
    "    until explicitly written to disk, a hashing function is used to \n",
    "    generate it's name in the virtual file system. This way similar array data is simply \n",
    "    overwritten in the file system.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.array \n",
    "          Numpy array to be hashed.\n",
    "          \n",
    "    Returns \n",
    "    -------\n",
    "    : str \n",
    "      Generated hash string \n",
    "    \n",
    "    \"\"\"\n",
    "    if isinstance(arr, GeoDataset):\n",
    "        arr = arr.read_array()\n",
    "    \n",
    "    string = \"shape={}\".format(arr.shape)\n",
    "    string += str(np.round(arr,1))\n",
    "    string += str(round(np.min(arr), 1))\n",
    "    string += str(round(np.max(arr), 1))\n",
    "    string += str(round(np.sum(arr), 1))\n",
    "    \n",
    "    sha1 = hashlib.sha1(string.replace(' ', '').replace('\\n','').encode()).hexdigest()\n",
    "    return sha1\n",
    "\n",
    "\n",
    "def crop(cropfile, extents, use_latlon=True):\n",
    "    \"\"\"\n",
    "    Uses the virtual file system (http://www.gdal.org/gdal_virtual_file_systems.html)\n",
    "    to crop an image in memory. TODO: Make this less jank, super slow\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    cropfile : str\n",
    "               Path to file to crop\n",
    "    extents : list\n",
    "              list in the format: [ul y, ul x, lr y, lr x]\n",
    "    use_latlon : bool\n",
    "                 if True, extents are in lat lon ranges for bounding box, \n",
    "                 else they are in pixel ranges.\n",
    "                 \n",
    "    Returns\n",
    "    -------\n",
    "    : GeoDataset \n",
    "      The cropped file\n",
    "    \n",
    "    \"\"\"\n",
    "    # hash the image info to get the filename\n",
    "    filename = hash_dataset(cropfile)\n",
    "    \n",
    "    if use_latlon:\n",
    "        ul = np.asarray(cropfile.latlon_to_pixel(extents[0], extents[1]))\n",
    "        lr = np.asarray(cropfile.latlon_to_pixel(extents[2], extents[3]))\n",
    "        window_size = np.abs(ul-lr)\n",
    "        extents = [ul[0], ul[1], window_size[0], window_size[1]]\n",
    "    \n",
    "    \n",
    "#     clip_arr = cropfile.read_array()[ul[0]:lr[0],ul[1]:lr[1]]\n",
    "#     clip = array2raster(cropfile, clip_arr, os.path.join('/vsimem',hash_dataset(clip_arr)))\n",
    "    clip = gdal.Translate(path.join('/vsimem', filename), cropfile.file_name, srcWin=extents)\n",
    "    return GeoDataset(clip.GetDescription())\n",
    "\n",
    "\n",
    "def pixels_to_latlon(geodataset, locs):\n",
    "    \"\"\"\n",
    "    Converts a list of pixels into lat lon space given a reference \n",
    "    image. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    geodataset : GeoDataset \n",
    "                 Reference image with tranformation info for conversion\n",
    "    locs : list \n",
    "           list of x,y pixel pairs to convert\n",
    "           \n",
    "    Returns \n",
    "    -------\n",
    "    : list \n",
    "      List of lat lon pairs\n",
    "    \"\"\"\n",
    "    coords = []\n",
    "    for loc in locs:\n",
    "        coords.append(geodataset.pixel_to_latlon(loc[1], loc[0]))\n",
    "    return coords\n",
    "\n",
    "\n",
    "def to_geodataset(dataset):\n",
    "    \"\"\"\n",
    "    Simple function to convert between GDAL dataset and Plio \n",
    "    GeoDataset. TODO: The fact that this function exists tells me it \n",
    "    might be usful to consolidate the two data structures to \n",
    "    have similar interfaces. \n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    dataset : Dataset \n",
    "              GDAL Dataset to convert\n",
    "              \n",
    "    Returns \n",
    "    -------\n",
    "    : Geodataset\n",
    "      The converted GeoDataset\n",
    "    \"\"\"\n",
    "    if not isinstance(dataset, GeoDataset):\n",
    "        return GeoDataset(path.abspath(dataset.GetDescription()))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def array2raster(rasterfn, array, newRasterfn):\n",
    "    \"\"\"\n",
    "    Writes an array to a GeoDataset using another dataset as reference. Borrowed  \n",
    "    from: https://pcjericks.github.io/py-gdalogr-cookbook/raster_layers.html\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    rasterfn : str, GeoDataset\n",
    "               Dataset or path to the dataset to use as a reference. Geotransform \n",
    "               and spatial reference information is copied into the new image. \n",
    "               \n",
    "    array : np.array \n",
    "            Array to write \n",
    "            \n",
    "    newRasterfn : str \n",
    "                  Filename for new raster image \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    : GeoDataset \n",
    "      File handle for the new raster file\n",
    "      \n",
    "    \"\"\"\n",
    "    naxis = len(array.shape)\n",
    "    assert naxis == 2 or naxis == 3      \n",
    "    \n",
    "    if naxis == 2:\n",
    "        # exapnd the third dimension\n",
    "        array = array[:,:,None]\n",
    "    \n",
    "    nbands = array.shape[2]\n",
    "    \n",
    "    if isinstance(rasterfn, GeoDataset):\n",
    "        rasterfn = rasterfn.file_name\n",
    "    \n",
    "    raster = gdal.Open(rasterfn)\n",
    "    geotransform = raster.GetGeoTransform()\n",
    "    originX = geotransform[0]\n",
    "    originY = geotransform[3]\n",
    "    pixelWidth = geotransform[1]\n",
    "    pixelHeight = geotransform[5]\n",
    "    cols = array.shape[1]\n",
    "    rows = array.shape[0]\n",
    "\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    outRaster = driver.Create(newRasterfn, cols, rows, nbands, gdal.GDT_Float32)\n",
    "    outRaster.SetGeoTransform((originX, pixelWidth, 0, originY, 0, pixelHeight))\n",
    "    \n",
    "    for band in range(1,nbands+1):\n",
    "        outband = outRaster.GetRasterBand(band)\n",
    "        # Bands use indexing starting at 1\n",
    "        outband.WriteArray(array[:,:,band-1])\n",
    "        outband.FlushCache()\n",
    "    \n",
    "    outRasterSRS = osr.SpatialReference()\n",
    "    outRasterSRS.ImportFromWkt(raster.GetProjectionRef())\n",
    "    outRaster.SetProjection(outRasterSRS.ExportToWkt())\n",
    "    outRaster = None\n",
    "    return GeoDataset(newRasterfn)\n",
    "\n",
    "\n",
    "def get_band_columns(df):\n",
    "    \"\"\"\n",
    "    Returns a list of available bands given a dataframe \n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df : DataFrame \n",
    "         input dataframe \n",
    "         \n",
    "    Returns\n",
    "    -------\n",
    "    : list \n",
    "      List of available bands\n",
    "    \"\"\"\n",
    "    band_pattern = re.compile(\"^b([0-9]+)\")\n",
    "    bands = [column for column in df.columns if band_pattern.match(column)]\n",
    "    return bands\n",
    "\n",
    "    \n",
    "def write_array(dataset, array, out=None):\n",
    "    \"\"\"\n",
    "    Like array2raster but jankier. Probably shouldn't be used, will delete later.\n",
    "    \"\"\"\n",
    "    naxis = len(array.shape)\n",
    "    assert naxis == 2 or naxis == 3      \n",
    "    \n",
    "    if naxis == 2:\n",
    "        # exapnd the third dimension\n",
    "        array = array[:,:,None]\n",
    "    \n",
    "    nbands = array.shape[2]\n",
    "    \n",
    "    if nbands > dataset.nbands:\n",
    "        for i in range(nbands-dataset.nbands):\n",
    "            dataset.dataset.AddBand()\n",
    "    \n",
    "    if out:\n",
    "        # copy the file \n",
    "        new_dataset = gdal.Translate(out, dataset.file_name)\n",
    "        for band in range(nbands):\n",
    "            outBand = new_dataset.GetRasterBand(band+1)\n",
    "            outBand.WriteArray(array[:,:,band])\n",
    "        del new_dataset\n",
    "        return GeoDataset(out)\n",
    "    \n",
    "    # Else use virtual filesystem\n",
    "    temp = gdal.Translate('/vsimem/temp', dataset.file_name)\n",
    "    for band in range(nbands):\n",
    "        outBand = temp.GetRasterBand(band+1)\n",
    "        outBand.WriteArray(array[:,:,band])\n",
    "\n",
    "    # copy file into proper name and delete temp\n",
    "    del temp\n",
    "    return to_geodataset(gdal.Translate(path.join('/vsimem/', hash_dataset('/vsimem/temp')), '/vsimem/temp'))\n",
    "\n",
    "\n",
    "def df2gdal(df, roi=None):\n",
    "    \"\"\"\n",
    "    Opens all the file paths in a dataframe to GeoDatasets. Input dataframe is expected \n",
    "    to have all columns mimicking that of the postgres schema for landsat8. Specifically, it will \n",
    "    only convert file names with columns b<#> where # is a band number 1-11. Should be upgraded soon \n",
    "    for other datasets. \n",
    "\n",
    "    This should be run before any computational function can be used on the Dataframe.\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df : DataFrame \n",
    "         input dataframe \n",
    "    roi : list \n",
    "          Region on interest expressed as lat lon corners: [ul y, ul x, lr y, lr x]\n",
    "    \n",
    "    Returns \n",
    "    -------\n",
    "    : DataFrame \n",
    "      copy of the input dataframe with band columns replaced by GeoDataset objects\n",
    "    \"\"\"\n",
    "    def read_bands(row, roi=None):\n",
    "        band_pattern = re.compile(\"^b([0-9]+)\")\n",
    "        bands = [column for column in df.columns if band_pattern.match(column)]\n",
    "        for band in bands:\n",
    "            row[band] = GeoDataset(row[band])\n",
    "            if roi:\n",
    "                row[band] = crop(row[band], roi)\n",
    "        return row\n",
    "    \n",
    "    return df.apply(read_bands, axis=1, roi=roi)\n",
    "\n",
    "\n",
    "\n",
    "def df2radiance(df):\n",
    "    \"\"\"\n",
    "    Coverts the Geodatasets in the input dataframe to radiance. New columns \n",
    "    in the form b<#>_rad for each available band in the dataframe containing a \n",
    "    GeoDataset with radiance values.\n",
    "    \n",
    "    Only converts columns with the name b<#> where '#' is some band number. As\n",
    "    this currently only works with Landsat 8, # has to be 1-11. Also, the \n",
    "    multiplication and addition constants must be in the DataFrame using the \n",
    "    Standard name that is used in the meta file attached to the Landsat scene.\n",
    "    i.e. radiance_mult_band_<#> and radiance_add_band_<#>\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df : DataFrame \n",
    "         input dataframe\n",
    "         \n",
    "    Returns \n",
    "    -------\n",
    "    : DataFrame \n",
    "      A copy of the input DataFrame with new columns for radiance rasters \n",
    "    \"\"\"\n",
    "    def row_radiance(row):\n",
    "        for bandnum in range(1,12):            \n",
    "            band_col = 'b{}'.format(bandnum)\n",
    "            if not band_col in row.index:\n",
    "                continue\n",
    "            \n",
    "            if isinstance(row[band_col], str):\n",
    "                file = GeoDataset(row[band_col])\n",
    "            else:\n",
    "                file = row[band_col]\n",
    "            \n",
    "            arr = file.read_array()\n",
    "            mult = row['radiance_mult_band_{}'.format(bandnum)]\n",
    "            add = row['radiance_add_band_{}'.format(bandnum)]\n",
    "            rad_arr = (arr * float(mult)) + float(add)\n",
    "            \n",
    "            row[band_col+'_rad'] = array2raster(file, rad_arr, os.path.join('/vsimem',hash_dataset(rad_arr)))\n",
    "        return row\n",
    "    return df.apply(row_radiance, axis=1)\n",
    "\n",
    "\n",
    "def df2brightness(df, e=1):\n",
    "    \"\"\"\n",
    "    Coverts the Geodatasets in the input dataframe to brightness temps. New columns \n",
    "    in the form b<#>_bright_temp for each available band in the dataframe containing a \n",
    "    GeoDataset with radiance values.\n",
    "    \n",
    "    Only converts columns with the name b<#> where '#' is some band number. As\n",
    "    this currently only works with Landsat 8, # has to be 10 and/or 11. Also, the \n",
    "    k1 and k2 constants must be in the DataFrame using the \n",
    "    Standard name that is used in the meta file attached to the Landsat scene.\n",
    "    i.e. radiance_mult_band_<#> and radiance_add_band_<#>\n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    df : DataFrame \n",
    "         input dataframe\n",
    "         \n",
    "    Returns \n",
    "    -------\n",
    "    : DataFrame \n",
    "      A copy of the input DataFrame with new columns for brightness temp rasters \n",
    "    \"\"\"\n",
    "    def row_brightness(row, e):\n",
    "        C1 = 1.1910428e-16\n",
    "        C2 = 0.0143877513\n",
    "        for bandnum in range(10,12):\n",
    "            rad_col = 'b'+str(bandnum)+'_rad'\n",
    "            k1_const_col = 'k1_constant_band_{}'.format(bandnum) \n",
    "            k2_const_col = 'k2_constant_band_{}'.format(bandnum)\n",
    "            if not rad_col in row.index or not k1_const_col in row.index or not k2_const_col in row.index:\n",
    "                raise Exception('Input does not have the required columns')\n",
    "            \n",
    "            wvl = landsat8_wavelengths[bandnum]\n",
    "            rad_arr = row[rad_col].read_array()\n",
    "            k1_const = float(row[k1_const_col])\n",
    "            k2_const = float(row[k2_const_col])\n",
    "            \n",
    "            bright_arr = k2_const/np.log(((e*k1_const)/rad_arr)+1) \n",
    "            \n",
    "            row['b'+str(bandnum)+'_bright_temp'] = array2raster(row[rad_col], bright_arr, os.path.join('/vsimem',hash_dataset(bright_arr)))\n",
    "        return row\n",
    "    return df.apply(row_brightness, axis=1, e=e)\n",
    "\n",
    "\n",
    "def bright_temp_diff_df(df, thresh=1.5):\n",
    "    \"\"\"\n",
    "    Diffs the brightness temps for band 10 and 11. Any values in the diff array \n",
    "    with values above the given threshhold are flagged as anomolies. \n",
    "    \n",
    "    Input dataframe must have the columns b10_bright_temps and b11_bright_temps \n",
    "    computed before use. \n",
    "    \n",
    "    Output dataframe contains two new columns: bright_temp_diff and bright_temp_anomolies \n",
    "    for raster files containing the diff images and anomoly array respectively. \n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "         input dataframe \n",
    "    thresh : float \n",
    "             thresh to use in anomoly tagging\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    : Dataframe\n",
    "      Copy of input dataframe \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    if 'b10_bright_temp' not in df.columns or 'b11_bright_temp' not in df.columns:\n",
    "        raise Exception(\"Brightness values got b10 and b11 not in the dataframe\")\n",
    "    \n",
    "    def diff_row_brightness(row, thresh=1.5):\n",
    "        b10_bright_arr = row['b10_bright_temp'].read_array()\n",
    "        b11_bright_arr = row['b11_bright_temp'].read_array()\n",
    "        diff_arr = b10_bright_arr - b11_bright_arr\n",
    "        anomolies = np.empty(diff_arr.shape)\n",
    "        anomolies[:] = False\n",
    "        anomolies[np.isnan(diff_arr)] = np.nan\n",
    "        anomolies[diff_arr >= thresh] = True\n",
    "        row['bright_temp_diff'] = array2raster(row['b10_bright_temp'], diff_arr, os.path.join('/vsimem',hash_dataset(diff_arr)))\n",
    "        row['bright_temp_anomolies'] = array2raster(row['b10_bright_temp'], anomolies,  os.path.join('/vsimem',hash_dataset(anomolies)))\n",
    "        return row\n",
    "    return df.apply(diff_row_brightness, axis=1, thresh=thresh)\n",
    "\n",
    "\n",
    "def df2Geotiff(df):\n",
    "    \"\"\"\n",
    "    Flattens bands into a GeoTiff file.\n",
    "    \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def rst_df(df, ref_df, band=10, thresh=2.6):\n",
    "    \"\"\"\n",
    "    GSD edition \n",
    "    \n",
    "    Assumes df and ref_df are the same ROI with brightness temps computed. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    df : DataFrame\n",
    "         input dataframe with potential anomolies\n",
    "    ref_df : DataFrame \n",
    "             Dataframe used as reference, mean and standard deviation for a \n",
    "             time series is computed uses this DataFrame \n",
    "    band : int \n",
    "           Band to use for computing mean, std deviation and ALICE.\n",
    "           \n",
    "    Returns \n",
    "    -------\n",
    "    : DataFrame \n",
    "      Copy in input df with new columns ['rst', 'rst_anomolies']\n",
    "    \n",
    "    \"\"\"\n",
    "    get_col = {\n",
    "        10 : 'b10_bright_temp',\n",
    "        11 : 'b11_bright_temp'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        col = get_col[band]\n",
    "    except: \n",
    "        raise Exception('Band must be either 10 or 11')\n",
    "    \n",
    "    ref_images = ref_df[col]\n",
    "    ref_images = [image.read_array() for image in ref_images]\n",
    "    ref_arr = np.stack(ref_images, axis=2)\n",
    "    ref_mean = np.mean(ref_arr, axis=2)\n",
    "    ref_stddev = np.std(ref_arr, axis=2)\n",
    "    def rst_row(row, col, ref_mean, ref_stddev, thresh):\n",
    "        arr = row[col].read_array()\n",
    "        alice = (arr - ref_mean)/ref_stddev\n",
    "        anomolies = np.empty(alice.shape)\n",
    "        anomolies[:] = False\n",
    "        anomolies[np.isnan(alice)] = np.nan\n",
    "        anomolies[alice >= thresh] = True\n",
    "        row['rst'] = array2raster(row[col], alice, os.path.join('/vsimem', hash_dataset(alice)))\n",
    "        row['rst_anomolies'] = array2raster(row[col], anomolies, os.path.join('/vsimem', hash_dataset(anomolies)))\n",
    "        return row\n",
    "    return df.apply(rst_row, axis=1, col=col, ref_mean=ref_mean, ref_stddev=ref_stddev, thresh=thresh)\n",
    "\n",
    "\n",
    "def vast( tir_data, corners ):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # calculate non volcano area minimum and maximum values for comparison in volcano area\n",
    "    non_volc_area = tir_data.copy()\n",
    "    non_volc_min = non_volc_area.min()\n",
    "\n",
    "    non_volc_area[corners[0][0]:corners[1][0], corners[0][1]:corners[1][1]] = non_volc_min\n",
    "\n",
    "    non_volc_area_max = non_volc_area.max()\n",
    "\n",
    "    anomolies = np.empty( tir_data.shape )\n",
    "    anomolies[:] = False\n",
    "    # Is there any better way to do this to avoid iterating?\n",
    "    for column in range( corners[0][0], corners[1][0] ):\n",
    "        for row in range( corners[0][1], corners[1][1] ):\n",
    "            if tir_data[column, row] - average_surrounding( tir_data, column, row ) > non_volc_area_max:\n",
    "                anomolies[column, row] = True\n",
    "    return anomolies \n",
    "\n",
    "\n",
    "def average_surrounding( data, x, y, size = 1):\n",
    "    # slice area around pixel and nan the center point to remove it from mean calculation\n",
    "    sub_area = data[(x - size): (x + size + 1), (y - size) : (y + size + 1)]\n",
    "    sub_area[size][size] = np.nan\n",
    "    average = sub_area[~np.isnan(sub_area)].mean()\n",
    "    return average\n",
    "\n",
    "\n",
    "def animate_band(images, cmap='coolwarm'):\n",
    "    \"\"\"\n",
    "    Given a Series of images, draws an animation iterating over \n",
    "    images in the order they are presented in the Series. \n",
    "    \n",
    "    Parameters \n",
    "    ----------\n",
    "    images : Series, list \n",
    "             Series or list of GeoDataset images\n",
    "    cmap : cmap, str\n",
    "           Matplotlib color map for coloring\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    : FuncAnimation\n",
    "      Matplotlib Animation\n",
    "    \n",
    "    \"\"\"\n",
    "    imagelist = list(images)\n",
    "    arrlist = []\n",
    "    for im in imagelist:\n",
    "        arr = im.read_array()\n",
    "        arr[arr == 0] = np.nan\n",
    "        arrlist.append(arr)\n",
    "        \n",
    "    fig=plt.figure(clear=True, figsize=(15,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # make axesimage object\n",
    "    div = make_axes_locatable(ax)\n",
    "    cax = div.append_axes('right', '5%', '5%')\n",
    "    \n",
    "    # the vmin and vmax here are very important to get the color map correct\n",
    "    im = ax.imshow(arrlist[0], cmap=cmap) # Here make an AxesImage rather than contour\n",
    "    cb = fig.colorbar(im, cax=cax)\n",
    "    # im = plt.imshow(arrlist[0], cmap=cmap)\n",
    "    tx = ax.set_title(images.index[0])\n",
    "    \n",
    "    # function to update figure\n",
    "    def updatefig(j):\n",
    "        # set the data in the axesimage object\n",
    "        vmax = np.max(arrlist[j])\n",
    "        vmin = np.min(arrlist[j])\n",
    "\n",
    "        im.set_data(arrlist[j])\n",
    "        im.set_clim(vmin, vmax)\n",
    "        tx = ax.set_title(images.index[j])\n",
    "        return im\n",
    "    \n",
    "    # kick off the animation\n",
    "    ani = animation.FuncAnimation(fig, updatefig, frames=len(arrlist), interval=400, repeat_delay=1000)\n",
    "    return ani\n",
    "\n",
    "\n",
    "def pixels_to_latlon(geodata, locs):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    a = np.apply_along_axis(lambda x:geodata.forward_affine*x, 1,locs)\n",
    "    return np.asarray(geodata.coordinate_transformation.TransformPoints(a))[:,:2]\n",
    "\n",
    "\n",
    "def latlons_to_pixel(geodata, locs):\n",
    "    \"\"\"\n",
    "    Convert from lat/lon space to pixel space (i.e. sample/line).\n",
    "    Parameters\n",
    "    ----------\n",
    "    lat: float\n",
    "         Latitude to be transformed.\n",
    "    lon : float\n",
    "          Longitude to be transformed.\n",
    "    Returns\n",
    "    -------\n",
    "    x, y : tuple\n",
    "           (Sample, line) position corresponding to the given (latitude, longitude).\n",
    "    \"\"\"\n",
    "    a = np.asarray(geodata.inverse_coordinate_transformation.TransformPoints(locs))[:,:2]\n",
    "    return np.apply_along_axis(lambda x:geodata.inverse_affine * x, 1, a).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "def vast_df( df, tir_band, corners, size = 1 ):\n",
    "    \"\"\"\n",
    "    Vast computation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "         input dataframe with potential anomolies\n",
    "    tir : np.array \n",
    "          array containing thermal infrared DNs for computing nti\n",
    "    corners : tuple\n",
    "              tuple containing two coordinates the top left and bottom right coordinates of a volcano area.\n",
    "    size : int \n",
    "           defaults to 1. The amount of surrounding rings to calculate average of. At size 1 this is the 8 adjacent pixels.\n",
    "    Returns\n",
    "    -------\n",
    "    : np.array \n",
    "      Boolean array of pixels flagged as anomolies    \n",
    "    \"\"\"\n",
    "    def vast_row( row, tir_band, corners, size ):\n",
    "        # calculate non volcano area minimum and maximum values for comparison in volcano area\n",
    "        tir_arr = row[tir_band].read_array()\n",
    "        \n",
    "        anomolies, non_volc_area, tir_vast = vast( tir_arr, corners, size )\n",
    "        anomolies = array2raster(row[tir_band], anomolies, os.path.join('/vsimem',hash_dataset(anomolies)))\n",
    "        non_volc_area = array2raster(row[tir_band], non_volc_area, os.path.join('/vsimem',hash_dataset(non_volc_area)))\n",
    "        tir_vast = array2raster(row[tir_band], tir_vast, os.path.join('/vsimem',hash_dataset(tir_vast)))\n",
    "        \n",
    "        row['vast_anomolies'] = anomolies\n",
    "        row['non_volc_area'] = non_volc_area\n",
    "        row['tir_vast'] = tir_vast\n",
    "        return row\n",
    "    return df.apply(vast_row, axis=1, tir_band=tir_band, corners=corners, size=size)\n",
    "\n",
    "\n",
    "def vast( tir, corners, size ):\n",
    "    \"\"\"\n",
    "    Vast computation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tir : np.array \n",
    "          array containing thermal infrared DNs for computing nti\n",
    "    corners : tuple\n",
    "              tuple containing two coordinates the top left and bottom right coordinates of a volcano area.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    : np.array \n",
    "      Boolean array of pixels flagged as anomolies    \n",
    "    \"\"\"\n",
    "    # calculate non volcano area minimum and maximum values for comparison in volcano area\n",
    "    non_volc_area = tir.copy()\n",
    "    non_volc_min = non_volc_area.min()\n",
    "\n",
    "    non_volc_area[corners[0][0]:corners[1][0], corners[0][1]:corners[1][1]] = non_volc_min\n",
    "\n",
    "    non_volc_area_max = non_volc_area.max()\n",
    "    anomolies = np.empty( tir.shape )\n",
    "    anomolies[:] = False\n",
    "    # Is there any better way to do this to avoid iterating?\n",
    "    for column in range( corners[0][0], corners[1][0] ):\n",
    "        for row in range( corners[0][1], corners[1][1] ):\n",
    "            if ( tir[column][row] - average_surrounding( tir, column, row, size ) ) > non_volc_area_max:\n",
    "                anomolies[column][row] = True\n",
    "\n",
    "    return anomolies, non_volc_area, tir\n",
    "\n",
    "\n",
    "def get_avg_temps_at_altitude(temps, sea_level, alt=0):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    projected_filename = os.path.join('/vsimem',temps.file_name+sea_level.file_name)\n",
    "    des_srs = temps.dataset.GetProjection()\n",
    "    src_srs = sea_level.dataset.GetProjection()\n",
    "    options = gdal.WarpOptions(srcSRS=src_srs, dstSRS=des_srs)\n",
    "    re_proj = gdal.Warp(projected_filename, sea_level.file_name, options=options)\n",
    "    re_proj = gdal.Open(projected_filename, gdal.GF_Write)\n",
    "    re_proj.SetGeoTransform(temps.geotransform)\n",
    "    re_proj = to_geodataset(re_proj)\n",
    "    \n",
    "    sea_level_locs = np.argwhere(re_proj.read_array()<=alt)\n",
    "    latlons = pixels_to_latlon(re_proj, sea_level_locs)\n",
    "    pixels = latlons_to_pixel(temps, latlons)\n",
    "    y, x = pixels.T\n",
    "    \n",
    "    wy = np.logical_and(y>=0, y<temps.read_array().shape[0])\n",
    "    wx = np.logical_and(x>=0, x<temps.read_array().shape[1])\n",
    "    mask = np.logical_and(wy,wx)\n",
    "\n",
    "    arr = temps.read_array()\n",
    "    y,x = pixels[mask].T\n",
    "    mean_temp = np.mean(arr[y,x])\n",
    "    return mean_temp, y,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Connect directly for now until thin interface through Scott's restful service can me created\n",
    "engine = create_engine('postgresql://kelvin:1234@bayleef.wr.usgs.gov:54320/thermal')\n",
    "\n",
    "# Load all the things into memory\n",
    "# Use pre-baked view on postgres\n",
    "sql = \"select * from landsat_8_c1 order by time\"\n",
    "df = gpd.GeoDataFrame.from_postgis(sql, engine , geom_col='geom').set_index('landsat_scene_id')\n",
    "\n",
    "# get only night time images\n",
    "df = df[~df['is_daytime']]\n",
    "\n",
    "df.columns, df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# limit fields to fields we actually care about\n",
    "df = df[['geom', 'time', 'b6','b10','b11', \n",
    "    'radiance_add_band_7', 'radiance_mult_band_7',\n",
    "    'radiance_add_band_6', 'radiance_mult_band_6',\n",
    "    'radiance_add_band_11', 'radiance_mult_band_11',\n",
    "    'radiance_add_band_10', 'radiance_mult_band_10',\n",
    "    'k1_constant_band_10', 'k1_constant_band_11',\n",
    "    'k2_constant_band_10', 'k2_constant_band_11']]\n",
    "\n",
    "# Just brebake some df's by year in case we want to be more granular\n",
    "df2014 = df[df.time.between(date(2014,1,1), date(2015,1,1))]\n",
    "df2015 = df[df.time.between(date(2015,1,1), date(2016,1,1))]\n",
    "df2016 = df[df.time.between(date(2016,1,1), date(2017,1,1))]\n",
    "df2017 = df[df.time.between(date(2017,1,1), date(2018,1,1))]\n",
    "df2018 = df[df.time.between(date(2018,1,1), date(2019,1,1))]\n",
    "\n",
    "# Cropping is expesive because of file IO\n",
    "kilauea = [19.445, -155.321, 19.343,-155.164]\n",
    "random_ocean = [19.3476, -155.0799, 19.2982, -155.0064]\n",
    "\n",
    "data = df2gdal(df, kilauea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Run all the things\n",
    "data = df2radiance(data)\n",
    "data = df2brightness(data)\n",
    "data = modvolc_df(data, 'b6_rad', 'b11_rad', thresh=-.5)\n",
    "data = bright_temp_diff_df(data, thresh=1.5)\n",
    "data = vast_df( data, 'b11_rad', (( 140, 130),( 160, 150)), 10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfshow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# compare 2014 to 2017 data\n",
    "time_mask = data.time.between(date(2014,1,1), date(2015,1,1))\n",
    "ref_df = data[time_mask]\n",
    "\n",
    "time_mask = data.time.between(date(2015,1,1), date(2017,6,1))\n",
    "test_df = data[time_mask]\n",
    "\n",
    "rdf = rst_df(test_df, ref_df, band=11, thresh=2.0)\n",
    "dfshow(rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Had issue finding .tif file in same directory as notebook.\n",
    "test = GeoDataset('ASTGTM2_N19W156_dem.tif')\n",
    "water_level = crop(test, random_ocean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_test = df2gdal(df.iloc[:40], roi=random_ocean)\n",
    "temp_test = df2radiance(temp_test)\n",
    "temp_test = df2brightness(temp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(temp_test.iloc[3]['b10_bright_temp'].read_array(), cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temps = temp_test.iloc[3]['b10_bright_temp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps.dataset.SetGeoTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(water_level.read_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg, y,x = get_avg_temps_at_altitude(temps, water_level,alt=0)\n",
    "a = temps.read_array()\n",
    "a[y,x] = np.nan\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = temps.read_array()\n",
    "# a[y,x] = avg\n",
    "plt.imshow(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdal.Info(water_level.file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdal.Info(temps.file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(gdal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Script to be added to Jenkins for cronjob updates.\n",
    "# More testing must be done once the 'jupyter.wr.usgs.gov' environment is stable.\n",
    "\n",
    "# Note: Changes made to Bayleef files were in coordination with Kelvin, and are currently required to run this code.\n",
    "\n",
    "from bayleef import api as bapi\n",
    "from bayleef.scripts import cli\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "from datetime import date\n",
    "from sqlalchemy import *\n",
    "import pandas as pd\n",
    "\n",
    "\"\"\"\n",
    "    Rough Kilauea Lat/Lon Boundaries:\n",
    "    \n",
    "          [\n",
    "            -156.3848876953125,\n",
    "            18.888096683445923\n",
    "          ],\n",
    "          [\n",
    "            -154.64080810546872,\n",
    "            20.226120295836992\n",
    "          ]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def update_thermal_database():\n",
    "    \"\"\"\n",
    "    \n",
    "    Uses Bayleef api to pull data from a provided path, and subsequently upload the data.\n",
    "    \n",
    "    Jenkins should used to run this function on a schedule. \n",
    "    Placeholder for Jenkins job is \"HySPIRI_thermal_cronjob\"\n",
    "    (Specific scheduling to be determined)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    None\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    \n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Get today's date.\n",
    "    current_date = date.today()\n",
    "    \n",
    "    # Database URI:\n",
    "    database_string = 'postgresql://kelvin:1234@bayleef.wr.usgs.gov:54320/thermal'\n",
    "    \n",
    "    # Data info and path\n",
    "    dataset_string = 'LANDSAT_8_C1'\n",
    "    node_string = 'EE'\n",
    "    root_string = '/scratch/krodriguez/bayeef-data/'\n",
    "    \n",
    "    # over Hawaii \n",
    "    ll_dict = {\"longitude\": -156.3848876953125, \"latitude\": 18.888096683445923}\n",
    "    ur_dict = {\"longitude\": -154.64080810546872, \"latitude\": 20.226120295836992}\n",
    "    \n",
    "    # create database engine for query.\n",
    "    engine = create_engine(database_string)\n",
    "    \n",
    "    # Obtain the last entry added to the database.\n",
    "    last_update = pd.read_sql(\"SELECT max(time) FROM landsat_8.images;\", engine)['max'][0]\n",
    "\n",
    "    # Convert the date objects to strings in order to be fed into bayleef's \"search\" method.\n",
    "    last_update_string = last_update.strftime('%Y-%m-%d')\n",
    "\n",
    "    current_date_string = current_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Search for data using provided parameters.\n",
    "    data_dict = bapi.search(dataset_string, node_string, distance=100, ll=ll_dict, ur=ur_dict, \n",
    "                      start_date=last_update_string, end_date=current_date_string, where=None, \n",
    "                      max_results=50000, starting_number=1, sort_order=\"DESC\", extended=False, api_key=None)\n",
    "    \n",
    "    # Convert returned dictionary into json.\n",
    "    data_json = json.dumps(data_dict)\n",
    "    \n",
    "    # Download the proper files.\n",
    "    bapi.download(root_string, node_string, data_json)\n",
    "\n",
    "    # Information for database upload.\n",
    "    host = 'bayleef.wr.usgs.gov'\n",
    "    port = '54320'\n",
    "    user = 'kelvin'\n",
    "    password = '1234'\n",
    "    \n",
    "    # Finally, upload the data to the database.\n",
    "    bapi.upload_sql(database_string, 'LANDSAT_8_C1', root_string, host, port, user, password)\n",
    "        \n",
    "update_thermal_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thermal_alerts2",
   "language": "python",
   "name": "thermal_alerts2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
